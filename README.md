# Transformers(doÄŸal dil iÅŸlemeyle ilgili olan )

![image](https://github.com/user-attachments/assets/88f3517e-2de2-4456-8786-a1ffc31259b2)
Transformers kÃ¼tÃ¼phanesi, farklÄ± doÄŸal dil iÅŸleme (NLP), bilgisayar gÃ¶rÃ¼sÃ¼ (CV), ses iÅŸleme ve diÄŸer gÃ¶revler iÃ§in Ã§ok sayÄ±da model tÃ¼rÃ¼ sunar. AÅŸaÄŸÄ±da Transformers kÃ¼tÃ¼phanesindeki en yaygÄ±n model tÃ¼rlerini ve bunlarÄ±n hangi tÃ¼r gÃ¶revler iÃ§in kullanÄ±ldÄ±ÄŸÄ±nÄ± listeledim:

---

### **1. Encoder-Only Modeller**
Encoder tabanlÄ± modeller, giriÅŸ dizisini kodlamak ve temsiller Ã¼retmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. Genellikle sÄ±nÄ±flandÄ±rma, Ã¶zetleme veya bilgi Ã§Ä±karÄ±mÄ± gibi gÃ¶revler iÃ§in uygundur.

- **BERT (Bidirectional Encoder Representations from Transformers)**:
  - KullanÄ±m AlanlarÄ±: Metin sÄ±nÄ±flandÄ±rma, duygu analizi, soru cevaplama, Ã¶zetleme.
  - Ã–rnek Modeller: `bert-base-uncased`, `bert-large-cased`.

- **RoBERTa (Robustly Optimized BERT Pretraining Approach)**:
  - BERT'in optimize edilmiÅŸ bir versiyonudur.
  - KullanÄ±m AlanlarÄ±: Daha iyi performans iÃ§in BERT ile aynÄ± tÃ¼r gÃ¶revler.
  - Ã–rnek Modeller: `roberta-base`, `roberta-large`.

- **DistilBERT**:
  - BERT'in daha hafif ve hÄ±zlÄ± bir versiyonudur.
  - KullanÄ±m AlanlarÄ±: Daha az hesaplama gereksinimi olan gÃ¶revler.
  - Ã–rnek Modeller: `distilbert-base-uncased`.

- **ELECTRA**:
  - Maskeli dil modelleme yerine "ayrÄ±calÄ±klÄ± eÄŸitim" kullanÄ±r.
  - KullanÄ±m AlanlarÄ±: DÃ¼ÅŸÃ¼k maliyetli dil modelleme.
  - Ã–rnek Modeller: `google/electra-base-discriminator`.

---

### **2. Decoder-Only Modeller**
Decoder tabanlÄ± modeller, genellikle metin Ã¼retimi ve diyalog sistemleri gibi gÃ¶revler iÃ§in kullanÄ±lÄ±r.

- **GPT (Generative Pretrained Transformer)**:
  - KullanÄ±m AlanlarÄ±: Metin Ã¼retimi, tamamlamasÄ± ve yaratÄ±cÄ± iÃ§erik Ã¼retimi.
  - Ã–rnek Modeller: `gpt2`, `gpt-neo`, `gpt-3` (OpenAI API ile eriÅŸim).

- **GPT-2 ve GPT-3**:
  - GPTâ€™nin geniÅŸletilmiÅŸ ve daha gÃ¼Ã§lÃ¼ versiyonlarÄ±dÄ±r.
  - KullanÄ±m AlanlarÄ±: GeniÅŸ metin Ã¼retimi gÃ¶revleri.

- **DialoGPT**:
  - Ã–zellikle diyalog odaklÄ± gÃ¶revler iÃ§in eÄŸitilmiÅŸ bir GPT modeli.
  - KullanÄ±m AlanlarÄ±: Sohbet botlarÄ±.
  - Ã–rnek Modeller: `microsoft/DialoGPT-small`.

- **OPT (Open Pretrained Transformer)**:
  - Meta AI tarafÄ±ndan geliÅŸtirilmiÅŸ, GPT'ye benzer bir model.
  - KullanÄ±m AlanlarÄ±: Metin Ã¼retimi.

---

### **3. Encoder-Decoder Modeller**
Bu modeller, giriÅŸ dizisini kodlayarak bir baÄŸlam vektÃ¶rÃ¼ oluÅŸturur ve ardÄ±ndan bunu bir Ã§Ä±ktÄ± dizisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Ã‡eviri, Ã¶zetleme ve sÄ±ralama dÃ¶nÃ¼ÅŸtÃ¼rme gÃ¶revleri iÃ§in uygundur.

- **T5 (Text-to-Text Transfer Transformer)**:
  - TÃ¼m NLP gÃ¶revlerini bir "metinden-mete dÃ¶nÃ¼ÅŸtÃ¼rme" problemi olarak ele alÄ±r.
  - KullanÄ±m AlanlarÄ±: Ã‡eviri, Ã¶zetleme, soru yanÄ±tlama.
  - Ã–rnek Modeller: `t5-small`, `t5-large`.

- **BART (Bidirectional and Auto-Regressive Transformer)**:
  - Encoder-decoder mimarisiyle Ã¶zetleme ve yeniden dÃ¼zenleme gÃ¶revlerinde baÅŸarÄ±lÄ±dÄ±r.
  - KullanÄ±m AlanlarÄ±: Ã–zetleme, metin dÃ¼zeltme.
  - Ã–rnek Modeller: `facebook/bart-large`.

- **MarianMT**:
  - Ã‡ok dilli Ã§eviri iÃ§in tasarlanmÄ±ÅŸtÄ±r.
  - KullanÄ±m AlanlarÄ±: Metin Ã§evirisi.
  - Ã–rnek Modeller: `Helsinki-NLP/opus-mt-en-de`.

- **PEGASUS**:
  - Ã–zetleme iÃ§in Ã¶zelleÅŸmiÅŸ bir modeldir.
  - KullanÄ±m AlanlarÄ±: Uzun metinlerin Ã¶zetlenmesi.
  - Ã–rnek Modeller: `google/pegasus-large`.

---

### **4. Ã‡ok Modlu (Multimodal) Modeller**
Bu modeller, birden fazla veri tÃ¼rÃ¼yle (Ã¶rneÄŸin, metin ve gÃ¶rseller) Ã§alÄ±ÅŸabilir.

- **CLIP (Contrastive Languageâ€“Image Pretraining)**:
  - GÃ¶rsel ve metin eÅŸleÅŸtirme iÃ§in tasarlanmÄ±ÅŸtÄ±r.
  - KullanÄ±m AlanlarÄ±: GÃ¶rsel sÄ±nÄ±flandÄ±rma, aÃ§Ä±klama oluÅŸturma.
  - Ã–rnek Modeller: `openai/clip-vit-base-patch32`.

- **DALLÂ·E**:
  - Metinden gÃ¶rsel oluÅŸturma.
  - KullanÄ±m AlanlarÄ±: YaratÄ±cÄ± iÃ§erik Ã¼retimi.
  - Ã–rnek Modeller: OpenAI API ile eriÅŸim.

- **FLAVA**:
  - Metin ve gÃ¶rseller Ã¼zerinde aynÄ± anda Ã§alÄ±ÅŸan Ã§ok modlu bir modeldir.
  - KullanÄ±m AlanlarÄ±: Ã‡ok modlu gÃ¶revler (gÃ¶rsel + metin).

---

### **5. DiÄŸer Ã–zel Modeller**

- **XLM-R (XLM-RoBERTa)**:
  - Ã‡ok dilli BERT tÃ¼revidir.
  - KullanÄ±m AlanlarÄ±: Ã‡apraz dilli NLP gÃ¶revleri.
  - Ã–rnek Modeller: `xlm-roberta-base`.

- **BigBird**:
  - Uzun dizilerle Ã§alÄ±ÅŸmak iÃ§in optimize edilmiÅŸ bir modeldir.
  - KullanÄ±m AlanlarÄ±: Uzun dokÃ¼man Ã¶zetleme ve iÅŸleme.
  - Ã–rnek Modeller: `google/bigbird-roberta-base`.

- **Longformer**:
  - Uzun metinleri iÅŸlemek iÃ§in dikkat (attention) mekanizmasÄ±nÄ± optimize eder.
  - KullanÄ±m AlanlarÄ±: Uzun dokÃ¼man iÅŸlemleri.
  - Ã–rnek Modeller: `allenai/longformer-base-4096`.

- **Switch Transformer**:
  - Daha bÃ¼yÃ¼k dil modelleri iÃ§in geliÅŸtirilen bir yÃ¶ntemdir.
  - KullanÄ±m AlanlarÄ±: YÃ¼ksek performanslÄ± dil modelleme.

Bu modeller, belirli NLP gÃ¶revleri iÃ§in optimize edilmiÅŸ olup farklÄ± veri tÃ¼rleri ve uygulama senaryolarÄ±na gÃ¶re seÃ§ilir. ğŸ˜Š
